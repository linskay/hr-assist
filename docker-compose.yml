version: '3.8'

services:
  whisperx:
    build: ./whisperx-service
    container_name: whisperx
    ports:
      - "9100:9000"
    environment:
      - PYTHONUNBUFFERED=1
      - TRANSFORMERS_CACHE=/root/.cache/huggingface
      - HF_HOME=/root/.cache/huggingface
      - TORCH_HOME=/root/.cache/torch
    networks:
      - hr-assistant-network
    volumes:
      - whisperx_models:/root/.cache

  # console:
  #   image: minio/console:latest
  #   container_name: hr-assistant-console
  #   environment:
  #     - CONSOLE_MINIO_SERVER=http://minio:9000
  #     - CONSOLE_PBKDF_PASSPHRASE=change-me
  #     - CONSOLE_PBKDF_SALT=change-me
  #   depends_on:
  #     - minio
  #   ports:
  #     - "9090:9090"
  #   networks:
  #     - hr-assistant-network
  # PostgreSQL Database
  postgres:
    image: postgres:15
    container_name: hr-assistant-postgres
    environment:
      POSTGRES_DB: hr_assistant
      POSTGRES_USER: hr_user
      POSTGRES_PASSWORD: hr_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - hr-assistant-network

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: hr-assistant-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - hr-assistant-network

  # RabbitMQ Message Queue
  rabbitmq:
    image: rabbitmq:3-management
    container_name: hr-assistant-rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
      - ./rabbitmq/definitions.json:/opt/rabbitmq/definitions.json
    networks:
      - hr-assistant-network

  # MinIO Object Storage
  minio:
    image: minio/minio:latest
    container_name: hr-assistant-minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9002:9000"
      - "9003:9001"
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    networks:
      - hr-assistant-network

  # Backend Application
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: hr-assistant-backend
    environment:
      SPRING_DATASOURCE_URL: jdbc:postgresql://postgres:5432/hr_assistant
      SPRING_DATASOURCE_USERNAME: hr_user
      SPRING_DATASOURCE_PASSWORD: hr_password
      SPRING_DATA_REDIS_HOST: redis
      SPRING_RABBITMQ_HOST: rabbitmq
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
      SBERT_URL: http://sbert-embed:8080
    ports:
      - "8081:8080"
    volumes:
      - ./models:/opt/models:ro
      - backend_logs:/app/logs
    depends_on:
      - postgres
      - redis
      - rabbitmq
      - minio
      - sbert-embed
    networks:
      - hr-assistant-network

  # SBERT Embedding Service (all-MiniLM-L6-v2)
  sbert-embed:
    build:
      context: ./embed-service
      dockerfile: Dockerfile
    container_name: hr-assistant-sbert-embed
    image: hr-assist/sbert-embed:latest
    environment:
      - TRANSFORMERS_CACHE=/root/.cache/huggingface
      - HF_HOME=/root/.cache/huggingface
      - TORCH_HOME=/root/.cache/torch
    ports:
      - "8088:8080"
    networks:
      - hr-assistant-network
    restart: unless-stopped

  # LLM Service (llama.cpp - Mistral-7B-Instruct or LLaMA 2 7B, quantized GGUF)
  llm:
    build:
      context: ./llm-service
      dockerfile: Dockerfile
    container_name: hr-assistant-llm
    image: hr-assist/llm:latest
    environment:
      - LLM_MODEL_PATH=/models/mistral-7b-instruct.Q4_K_M.gguf
      - LLM_CTX_SIZE=2048
      - LLM_THREADS=6
    volumes:
      - ./models/llm:/models:ro
    ports:
      - "8090:8090"
    networks:
      - hr-assistant-network
    restart: unless-stopped

  # Video Antifraud Service (DeepFace + MediaPipe)
  video-antifraud:
    build:
      context: ./video-antifraud-service
      dockerfile: Dockerfile
    container_name: hr-assistant-video-antifraud
    image: hr-assist/video-antifraud:latest
    environment:
      - TRANSFORMERS_CACHE=/root/.cache/huggingface
      - HF_HOME=/root/.cache/huggingface
    ports:
      - "8091:8091"
    networks:
      - hr-assistant-network
    restart: unless-stopped

  # DetectGPT Service (roberta-base-openai-detector)
  detectgpt:
    build:
      context: ./detectgpt-service
      dockerfile: Dockerfile
    container_name: hr-assistant-detectgpt
    image: hr-assist/detectgpt:latest
    environment:
      - TRANSFORMERS_CACHE=/root/.cache/huggingface
      - HF_HOME=/root/.cache/huggingface
    ports:
      - "8092:8092"
    networks:
      - hr-assistant-network
    restart: unless-stopped

  # Frontend Application
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: hr-assistant-frontend
    ports:
      - "3000:80"
    depends_on:
      - backend
    networks:
      - hr-assistant-network

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: hr-assistant-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - hr-assistant-network

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: hr-assistant-grafana
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    ports:
      - "3001:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    networks:
      - hr-assistant-network

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: hr-assistant-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - hr-assistant-network

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: hr-assistant-kibana
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    networks:
      - hr-assistant-network

  # Logstash
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: hr-assistant-logstash
    volumes:
      - ./monitoring/logstash/pipeline:/usr/share/logstash/pipeline
      - ./monitoring/logstash/config:/usr/share/logstash/config
    ports:
      - "5044:5044"
    depends_on:
      - elasticsearch
    networks:
      - hr-assistant-network

volumes:
  postgres_data:
  redis_data:
  rabbitmq_data:
  minio_data:
  backend_logs:
  prometheus_data:
  grafana_data:
  elasticsearch_data:
  whisperx_models:

networks:
  hr-assistant-network:
    driver: bridge
